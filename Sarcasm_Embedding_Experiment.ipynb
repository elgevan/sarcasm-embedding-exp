{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK3dBLDrNUr5"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This Colab explores the impact of different text embeddings on generalization in a text classification task. I train models on a 1k labelled data points and test on 25k (data from [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/code/nilanml/detecting-sarcasm-using-different-embeddings)) to emphasize the role of data quality over extensive model tuning. I compare the accuracy scores achieved with various embeddings to identify which best captures the underlying data structure and promotes generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJmFFpmQc5Rn"
      },
      "source": [
        "# Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhIDYTyRupK"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvIz1DNiGaJP"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gensim.downloader as api\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cFkirRvRyGG"
      },
      "source": [
        "## Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHD5Py-MKhxR"
      },
      "outputs": [],
      "source": [
        "# Pull data from kaggle\n",
        "path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
        "df = pd.read_json(path + '/Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "df = df[['headline', 'is_sarcastic']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['headline'], df['is_sarcastic'], test_size=0.9625, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2iM6DJUR0n7"
      },
      "source": [
        "## Define Models and Hyperparameter Search Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwc2wuWbKmT1"
      },
      "outputs": [],
      "source": [
        "# Define models and their parameter grids\n",
        "models = {\n",
        "    \"Random Forest\": {\n",
        "        \"model\": RandomForestClassifier(random_state=42),\n",
        "        \"param_grid\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    \"Logistic Regression\": {\n",
        "        \"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "        \"param_grid\": {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear', 'saga']\n",
        "        }\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        \"model\": GradientBoostingClassifier(random_state=42),\n",
        "        \"param_grid\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 1.0],\n",
        "            'max_depth': [3, 4, 5],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to train and evaluate a model\n",
        "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=10,\n",
        "        cv=5,\n",
        "        scoring=\"accuracy\",\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "    accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "    print(f\"Accuracy of {model.__class__.__name__}: {accuracy}\")\n",
        "    print(f\"Best parameters: {random_search.best_params_}\")\n",
        "    return accuracy, best_model\n",
        "\n",
        "# Initialize a list to store model results\n",
        "model_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6JSOxy7R9bm"
      },
      "source": [
        "# Tokenizer Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh10ZadiKsNW"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing\n",
        "tokenizer = Tokenizer(num_words=40000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, padding='post', maxlen=384)\n",
        "X_test_padded = pad_sequences(X_test_seq, padding='post', maxlen=384)\n",
        "\n",
        "# --- Tokenized and Padded Data ---\n",
        "for model_name, model_data in models.items():\n",
        "    print(f\"Training and evaluating {model_name}...\")\n",
        "    accuracy, best_model = train_and_evaluate_model(\n",
        "        model_data[\"model\"],\n",
        "        model_data[\"param_grid\"],\n",
        "        X_train_padded,\n",
        "        y_train,\n",
        "        X_test_padded,\n",
        "        y_test\n",
        "    )\n",
        "    model_results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"embedding_strategy\": \"Tokenized and Padded\",\n",
        "        \"accuracy\": accuracy,\n",
        "        \"best_model\": best_model\n",
        "    })\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEi-J6OOSBsf"
      },
      "source": [
        "# Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtywFwrlK06D"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Embeddings\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def word2vec_embedding(texts, word_vectors):\n",
        "    \"\"\"Embeds text using pre-trained Word2Vec.\"\"\"\n",
        "    sentences = [text.split() for text in texts]\n",
        "    embedded_texts = []\n",
        "    for sentence in sentences:\n",
        "        sentence_embedding = []\n",
        "        for word in sentence:\n",
        "            if word in word_vectors.key_to_index:\n",
        "                sentence_embedding.append(word_vectors[word])\n",
        "        if sentence_embedding:\n",
        "            embedded_texts.append(np.mean(np.array(sentence_embedding), axis=0))\n",
        "        else:\n",
        "            embedded_texts.append(np.zeros(word_vectors.vector_size))\n",
        "    return np.array(embedded_texts)\n",
        "\n",
        "X_train_embedded = word2vec_embedding(X_train, word_vectors)\n",
        "X_test_embedded = word2vec_embedding(X_test, word_vectors)\n",
        "\n",
        "for model_name, model_data in models.items():\n",
        "    print(f\"Training and evaluating {model_name}...\")\n",
        "    accuracy, best_model = train_and_evaluate_model(\n",
        "        model_data[\"model\"],\n",
        "        model_data[\"param_grid\"],\n",
        "        X_train_embedded,\n",
        "        y_train,\n",
        "        X_test_embedded,\n",
        "        y_test\n",
        "    )\n",
        "    model_results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"embedding_strategy\": \"Word2Vec\",\n",
        "        \"accuracy\": accuracy,\n",
        "        \"best_model\": best_model\n",
        "    })\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-DnFF0OSFg5"
      },
      "source": [
        "# SentenceTransformer Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f9CsnxwaBzpD"
      },
      "outputs": [],
      "source": [
        "# SentenceTransformer Embeddings\n",
        "sentence_models = [\n",
        "    \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
        "    \"all-distilroberta-v1\",\n",
        "    \"all-MiniLM-L6-v2\",\n",
        "    \"all-mpnet-base-v2\",\n",
        "    \"bert-base-uncased\",\n",
        "    \"jinaai/jina-embeddings-v3\",\n",
        "    \"multi-qa-mpnet-base-dot-v1\"\n",
        "]\n",
        "\n",
        "for sentence_model_name in sentence_models:\n",
        "    print(\"-\" * 120)\n",
        "    print(f\"Using SentenceTransformer model: {sentence_model_name}\")\n",
        "    model = SentenceTransformer(sentence_model_name, trust_remote_code=True)\n",
        "    X_train_embeddings = model.encode(X_train.tolist())\n",
        "    X_test_embeddings = model.encode(X_test.tolist())\n",
        "    for model_name, model_data in models.items():\n",
        "        print(f\"Training and evaluating {model_name}...\")\n",
        "        accuracy, best_model = train_and_evaluate_model(\n",
        "            model_data[\"model\"],\n",
        "            model_data[\"param_grid\"],\n",
        "            X_train_embeddings,\n",
        "            y_train,\n",
        "            X_test_embeddings,\n",
        "            y_test\n",
        "        )\n",
        "        model_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"embedding_strategy\": f\"SentenceTransformer: {sentence_model_name}\",\n",
        "            \"accuracy\": accuracy,\n",
        "            \"best_model\": best_model\n",
        "        })\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Create a pandas DataFrame from the model_results list\n",
        "results_df = pd.DataFrame(model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral Embeddings"
      ],
      "metadata": {
        "id": "TeatUwpc4ee3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mistral_embedding(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Generates an embedding for the input text using the hidden layers of a Mistral 7B model.\n",
        "\n",
        "    Args:\n",
        "        text: The input text (string).\n",
        "        model: The Mistral model.\n",
        "        tokenizer: The Mistral tokenizer.\n",
        "        device: The device to run the model on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        A PyTorch tensor representing the embedding of the text.\n",
        "        Returns None if the text is empty after tokenization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Check if input_ids is empty\n",
        "    if inputs.input_ids.size(1) == 0:\n",
        "        print(\"Warning: Empty input after tokenization.\")\n",
        "        return None\n",
        "\n",
        "    # Get the model's output, including hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Extract the hidden states\n",
        "    hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "    # Take the mean of the hidden states across tokens\n",
        "    embedding = torch.mean(hidden_states, dim=1)\n",
        "\n",
        "    return embedding.squeeze()"
      ],
      "metadata": {
        "id": "NU520goT4dvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Mistral 7B model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load the model (with quantization)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SwjzqMcd4kLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mistral LLM Embeddings\n",
        "X_train_mistral_embeddings = []\n",
        "X_test_mistral_embeddings = []\n",
        "\n",
        "for text in X_train:\n",
        "    embedding = get_mistral_embedding(text, model, tokenizer, device=None)\n",
        "    if embedding is not None:\n",
        "        X_train_mistral_embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "for text in X_test:\n",
        "    embedding = get_mistral_embedding(text, model, tokenizer, device=None)\n",
        "    if embedding is not None:\n",
        "        X_test_mistral_embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "X_train_mistral_embeddings = np.array(X_train_mistral_embeddings)\n",
        "X_test_mistral_embeddings = np.array(X_test_mistral_embeddings)\n",
        "\n",
        "for model_name, model_data in models.items():\n",
        "    print(f\"Training and evaluating {model_name}...\")\n",
        "    accuracy, best_model = train_and_evaluate_model(\n",
        "        model_data[\"model\"],\n",
        "        model_data[\"param_grid\"],\n",
        "        X_train_mistral_embeddings,\n",
        "        y_train,\n",
        "        X_test_mistral_embeddings,\n",
        "        y_test\n",
        "    )\n",
        "    model_results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"embedding_strategy\": \"Mistral LLM Embeddings\",\n",
        "        \"accuracy\": accuracy,\n",
        "        \"best_model\": best_model\n",
        "    })\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "-Wd6G7Pu4pOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WwhIDYTyRupK",
        "9cFkirRvRyGG",
        "O2iM6DJUR0n7"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNNWkEbg/pTY8MmOsz94Pe9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}