{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMEfPDPkJyQHiHI09pQIgGA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "from huggingface_hub import login\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import userdata\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "DjLy8sbl051G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull data from kaggle\n",
        "path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
        "df = pd.read_json(path + '/Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "df = df[['headline', 'is_sarcastic']]"
      ],
      "metadata": {
        "id": "THUmsnXW007M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Model and Tokenizer (Mistral 7B) ---\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Add pad_token if not defined by the tokenizer (important for batching)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 2. Prompt Template ---\n",
        "prompt_template = \"\"\"\n",
        "[INST]\n",
        "Analyze the following headline and determine if it is sarcastic/parody (1) or real news (0).\n",
        "\n",
        "Here are two examples:\n",
        "\n",
        "Headline: Former Versace store clerk sues over secret 'black code' for minority shoppers\n",
        "Classification: 0 (Real News)\n",
        "\n",
        "Headline: Boehner Just Wants Wife To Listen, Not Come Up With Alternative Debt-Reduction Ideas\n",
        "Classification: 1 (Sarcastic/Parody)\n",
        "\n",
        "Now analyze this headline:\n",
        "\n",
        "Headline: {headline}\n",
        "\n",
        "Respond only with the number 1 or 0.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. Batching Function ---\n",
        "def create_batches(data, batch_size, prompt_template, tokenizer):\n",
        "    batches = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_headlines = data[i:i + batch_size]\n",
        "        batch_prompts = [prompt_template.format(headline=h) for h in batch_headlines]\n",
        "\n",
        "        # Tokenize the entire batch at once\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",  # Pad to the maximum length in the batch\n",
        "            truncation=True,\n",
        "            max_length=512, # Control max input length\n",
        "        ).to(model.device)\n",
        "\n",
        "        batches.append((inputs, batch_headlines))\n",
        "    return batches\n",
        "\n",
        "# --- 5. Inference Function (Modified for Batching) ---\n",
        "def classify_batch(batch, model, tokenizer):\n",
        "    inputs, batch_headlines = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode all generated texts in the batch\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    results = []\n",
        "    for headline, generated_text in zip(batch_headlines, generated_texts):\n",
        "        # Extract classification\n",
        "        parts = generated_text.split(\"[/INST]\")[1].strip().split(\":\", 1)\n",
        "        classification = parts[0].strip().capitalize()\n",
        "        results.append({\n",
        "            \"headline\": headline,\n",
        "            \"classification\": classification\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- 6. Main Loop (Batch Processing) ---\n",
        "batch_size = 8  # Adjust as needed based on your GPU memory\n",
        "batches = create_batches(df['headline'], batch_size, prompt_template, tokenizer)\n",
        "\n",
        "all_results = []\n",
        "for batch in batches:\n",
        "    batch_results = classify_batch(batch, model, tokenizer)\n",
        "    all_results.extend(batch_results)\n",
        "\n",
        "# --- 7. Save Results ---\n",
        "results_df = pd.DataFrame(all_results)\n",
        "print(\"Batch classification complete.\")"
      ],
      "metadata": {
        "id": "HT83LD9SmwWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df['is_sarcastic_pred'] = results_df['classification'].str.extract(r\"^([01])\").astype(int)"
      ],
      "metadata": {
        "id": "ezoBMP5YyrFB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(df['is_sarcastic'], results_df['is_sarcastic_pred'])\n",
        "print(f\"Accuracy of the classifier: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bls5CS_S0Qdk",
        "outputId": "189b327b-034d-43f5-e755-d18eabd98377"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the classifier: 0.5749372870567974\n"
          ]
        }
      ]
    }
  ]
}